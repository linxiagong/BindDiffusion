{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An easy-to-use notebook for beginners:\n",
    "1. Define ImageBind model\n",
    "2. Define Diffusion model\n",
    "3. Image-conditioned audio generation\n",
    "4. Image-conditioned audio editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" importing modules\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from easydict import EasyDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ImageBind.data as data\n",
    "from ImageBind.models import imagebind_model\n",
    "from ImageBind.models.imagebind_model import ModalityType\n",
    "\n",
    "from ldm.models.diffusion.ddpm import ImageEmbeddingConditionedLatentDiffusion\n",
    "from ldm.models.diffusion.ddim import DDIMSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define ImageBind model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binder:\n",
    "    \"\"\" Wrapper for ImageBind model\n",
    "    \"\"\"\n",
    "    def __init__(self, pth_path, device='cuda'):\n",
    "        self.model = imagebind_model.imagebind_huge_pth(pretrained=True, pth_path=pth_path)\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.data_process_dict = {ModalityType.TEXT: data.load_and_transform_text,\n",
    "                                  ModalityType.VISION: data.load_and_transform_vision_data,\n",
    "                                  ModalityType.AUDIO: data.load_and_transform_audio_data}\n",
    "\n",
    "    def run(self, ctype, cpaths, post_process=False):\n",
    "        \"\"\" ctype: str\n",
    "            cpaths: list[str]\n",
    "        \"\"\"\n",
    "        inputs = {ctype: self.data_process_dict[ctype](cpaths, self.device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(inputs, post_process=post_process)\n",
    "\n",
    "        return embeddings[ctype]\n",
    "\n",
    "device = 'cuda'\n",
    "binder = Binder(pth_path=\"ImageBind/.checkpoints/imagebind_huge.pth\", device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Diffusion model (AudioLDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# options\n",
    "opt = EasyDict(config = 'stablediffusion/configs/stable-diffusion/v2-1-stable-unclip-h-inference.yaml',\n",
    "               device = device,\n",
    "               ckpt = 'stablediffusion/checkpoints/sd21-unclip-h.ckpt',\n",
    "               C = 4,\n",
    "               H = 768,\n",
    "               W = 768,\n",
    "               f = 8,\n",
    "               steps = 50, \n",
    "               n_samples = 1,\n",
    "               scale = 20,\n",
    "               ddim_eta = 0,\n",
    "               )\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "batch_size = opt.n_samples\n",
    "\n",
    "# prepare diffusion model for audio\n",
    "# if xtype == 'audio':\n",
    "#             x = net.audioldm_decode(z)\n",
    "#             x = self.mel_spectrogram_to_waveform(x)\n",
    "#             return x\n",
    "    # def mel_spectrogram_to_waveform(self, mel):\n",
    "    #     # Mel: [bs, 1, t-steps, fbins]\n",
    "    #     if len(mel.size()) == 4:\n",
    "    #         mel = mel.squeeze(1)\n",
    "    #     mel = mel.permute(0, 2, 1)\n",
    "    #     waveform = self.net.audioldm.vocoder(mel)\n",
    "    #     waveform = waveform.cpu().detach().numpy()\n",
    "    #     return waveform\n",
    "\n",
    "from audioldm import LatentDiffusion\n",
    "# No normalization here\n",
    "model = LatentDiffusion(**config[\"model\"][\"params\"])\n",
    "checkpoint = torch.load(opt.ckpt, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "model.to(opt.device)\n",
    "model.eval()\n",
    "\n",
    "model.cond_stage_model.embed_mode = \"text\"\n",
    "\n",
    "# -----\n",
    "# model = ImageEmbeddingConditionedLatentDiffusion(**config.model['params'])\n",
    "# pl_sd = torch.load(opt.ckpt, map_location=\"cpu\")\n",
    "# sd = pl_sd[\"state_dict\"]\n",
    "# model.load_state_dict(sd, strict=False)\n",
    "# model.to(opt.device)\n",
    "# model.eval()\n",
    "\n",
    "# sampler = DDIMSampler(model, device=opt.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1.\n",
    "\n",
    "def load_audio(path):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image-conditioned audio generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ['colorful, DSLR quality, clear, vivid'] * batch_size    # you may add extra descriptions you like here\n",
    "\n",
    "c_adm = binder.run(ctype=ModalityType.VISION, cpaths=['ImageBind/.assets/bird_audio.wav'], post_process=False)\n",
    "# c_adm = binder.run(ctype='audio', cpaths=['ImageBind/.assets/bird_audio.wav'], post_process=False)\n",
    "c_adm = c_adm / c_adm.norm() * 20   # a norm of 20 typically gives better result \n",
    "c_adm = torch.cat([c_adm] * batch_size, dim=0)\n",
    "\n",
    "with torch.no_grad(), torch.autocast('cuda'):\n",
    "    \n",
    "    c_adm, noise_level_emb = model.noise_augmentor(c_adm, noise_level=torch.zeros(batch_size).long().to(c_adm.device))\n",
    "    c_adm = torch.cat((c_adm, noise_level_emb), 1)\n",
    "\n",
    "    uc = model.get_learned_conditioning(batch_size * [\"text, watermark, blurry, number\"])    # negative prompts\n",
    "    uc = {\"c_crossattn\": [uc], \"c_adm\": torch.zeros_like(c_adm)}\n",
    "    c = {\"c_crossattn\": [model.get_learned_conditioning(prompts)], \"c_adm\": c_adm}\n",
    "\n",
    "    # samples, _ = sampler.sample(S=opt.steps,\n",
    "    #                             conditioning=c,\n",
    "    #                             batch_size=batch_size,\n",
    "    #                             shape=shape,\n",
    "    #                             verbose=False,\n",
    "    #                             unconditional_guidance_scale=opt.scale,\n",
    "    #                             unconditional_conditioning=uc,\n",
    "    #                             eta=opt.ddim_eta,\n",
    "    #                             x_T=None)\n",
    "    # --- generate waveform ---\n",
    "    # another usage is transfer_style\n",
    "    ddim_steps=200\n",
    "    duration=10\n",
    "    batchsize=1\n",
    "    guidance_scale=2.5\n",
    "    n_candidate_gen_per_text=3\n",
    "    waveform = None\n",
    "    batch = make_batch_for_text_to_audio(text, waveform=waveform, batchsize=batchsize)\n",
    "    \n",
    "    waveform = model.generate_sample(\n",
    "            [batch],\n",
    "            unconditional_guidance_scale=guidance_scale,\n",
    "            ddim_steps=ddim_steps,\n",
    "            n_candidate_gen_per_text=n_candidate_gen_per_text,\n",
    "            duration=duration,\n",
    "        )\n",
    "\n",
    "# x_samples = model.decode_first_stage(samples)\n",
    "# x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "# plt.imshow(x_samples[0].permute(1,2,0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_audio(\n",
    "    audioldm,\n",
    "    text,\n",
    "    original_audio_file_path = None,\n",
    "    seed=42,\n",
    "    ddim_steps=200,\n",
    "    duration=10,\n",
    "    batchsize=1,\n",
    "    guidance_scale=2.5,\n",
    "    n_candidate_gen_per_text=3,\n",
    "    config=None,\n",
    "):\n",
    "    seed_everything(int(seed))\n",
    "    waveform = None\n",
    "    if(original_audio_file_path is not None):\n",
    "        waveform = read_wav_file(original_audio_file_path, int(duration * 102.4) * 160)\n",
    "        \n",
    "    batch = make_batch_for_text_to_audio(text, waveform=waveform, batchsize=batchsize)\n",
    "\n",
    "    audioldm.latent_t_size = duration_to_latent_t_size(duration)\n",
    "    \n",
    "    if(waveform is not None):\n",
    "        print(\"Generate audio that has similar content as %s\" % original_audio_file_path)\n",
    "        audioldm = set_cond_audio(audioldm)\n",
    "    else:\n",
    "        print(\"Generate audio using text %s\" % text)\n",
    "        audioldm = set_cond_text(audioldm)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        waveform = audioldm.generate_sample(\n",
    "            [batch],\n",
    "            unconditional_guidance_scale=guidance_scale,\n",
    "            ddim_steps=ddim_steps,\n",
    "            n_candidate_gen_per_text=n_candidate_gen_per_text,\n",
    "            duration=duration,\n",
    "        )\n",
    "    return waveform\n",
    "\n",
    "audioldm = build_model(model_name=args.model_name)\n",
    "waveform = text_to_audio(\n",
    "        audioldm,\n",
    "        text,\n",
    "        args.file_path,\n",
    "        random_seed,\n",
    "        duration=duration,\n",
    "        guidance_scale=guidance_scale,\n",
    "        ddim_steps=args.ddim_steps,\n",
    "        n_candidate_gen_per_text=n_candidate_gen_per_text,\n",
    "        batchsize=args.batchsize,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
