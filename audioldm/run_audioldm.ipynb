{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run vanilla AudioLDM\n",
    "This is to run vanilla AudioLDM: [github](https://github.com/haoheliu/AudioLDM/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. AudioLDM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(DDPM):\n",
    "    \"\"\"main class\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device=\"cuda\",\n",
    "        first_stage_config=None,\n",
    "        cond_stage_config=None,\n",
    "        num_timesteps_cond=None,\n",
    "        cond_stage_key=\"image\",\n",
    "        cond_stage_trainable=False,\n",
    "        concat_mode=True,\n",
    "        cond_stage_forward=None,\n",
    "        conditioning_key=None,\n",
    "        scale_factor=1.0,\n",
    "        scale_by_std=False,\n",
    "        base_learning_rate=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.learning_rate = base_learning_rate\n",
    "        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n",
    "        self.scale_by_std = scale_by_std\n",
    "        assert self.num_timesteps_cond <= kwargs[\"timesteps\"]\n",
    "        # for backwards compatibility after implementation of DiffusionWrapper\n",
    "        if conditioning_key is None:\n",
    "            conditioning_key = \"concat\" if concat_mode else \"crossattn\"\n",
    "        if cond_stage_config == \"__is_unconditional__\":\n",
    "            conditioning_key = None\n",
    "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
    "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
    "        super().__init__(conditioning_key=conditioning_key, *args, **kwargs)\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cond_stage_trainable = cond_stage_trainable\n",
    "        self.cond_stage_key = cond_stage_key\n",
    "        self.cond_stage_key_orig = cond_stage_key\n",
    "        try:\n",
    "            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n",
    "        except:\n",
    "            self.num_downs = 0\n",
    "        if not scale_by_std:\n",
    "            self.scale_factor = scale_factor\n",
    "        else:\n",
    "            self.register_buffer(\"scale_factor\", torch.tensor(scale_factor))\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        self.instantiate_cond_stage(cond_stage_config)\n",
    "        self.cond_stage_forward = cond_stage_forward\n",
    "        self.clip_denoised = False\n",
    "\n",
    "    def make_cond_schedule(\n",
    "        self,\n",
    "    ):\n",
    "        self.cond_ids = torch.full(\n",
    "            size=(self.num_timesteps,),\n",
    "            fill_value=self.num_timesteps - 1,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        ids = torch.round(\n",
    "            torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)\n",
    "        ).long()\n",
    "        self.cond_ids[: self.num_timesteps_cond] = ids\n",
    "\n",
    "    def register_schedule(\n",
    "        self,\n",
    "        given_betas=None,\n",
    "        beta_schedule=\"linear\",\n",
    "        timesteps=1000,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        cosine_s=8e-3,\n",
    "    ):\n",
    "        super().register_schedule(\n",
    "            given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s\n",
    "        )\n",
    "\n",
    "        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n",
    "        if self.shorten_cond_schedule:\n",
    "            self.make_cond_schedule()\n",
    "\n",
    "    def instantiate_first_stage(self, config):\n",
    "        model = instantiate_from_config(config)\n",
    "        self.first_stage_model = model.eval()\n",
    "        self.first_stage_model.train = disabled_train\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def instantiate_cond_stage(self, config):\n",
    "        if not self.cond_stage_trainable:\n",
    "            if config == \"__is_first_stage__\":\n",
    "                print(\"Using first stage also as cond stage.\")\n",
    "                self.cond_stage_model = self.first_stage_model\n",
    "            elif config == \"__is_unconditional__\":\n",
    "                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n",
    "                self.cond_stage_model = None\n",
    "                # self.be_unconditional = True\n",
    "            else:\n",
    "                model = instantiate_from_config(config)\n",
    "                self.cond_stage_model = model.eval()\n",
    "                self.cond_stage_model.train = disabled_train\n",
    "                for param in self.cond_stage_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            assert config != \"__is_first_stage__\"\n",
    "            assert config != \"__is_unconditional__\"\n",
    "            model = instantiate_from_config(config)\n",
    "            self.cond_stage_model = model\n",
    "        self.cond_stage_model = self.cond_stage_model.to(self.device)\n",
    "\n",
    "    def get_first_stage_encoding(self, encoder_posterior):\n",
    "        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n",
    "            z = encoder_posterior.sample()\n",
    "        elif isinstance(encoder_posterior, torch.Tensor):\n",
    "            z = encoder_posterior\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\"\n",
    "            )\n",
    "        return self.scale_factor * z\n",
    "\n",
    "    def get_learned_conditioning(self, c):\n",
    "        if self.cond_stage_forward is None:\n",
    "            if hasattr(self.cond_stage_model, \"encode\") and callable(\n",
    "                self.cond_stage_model.encode\n",
    "            ):\n",
    "                c = self.cond_stage_model.encode(c)\n",
    "                if isinstance(c, DiagonalGaussianDistribution):\n",
    "                    c = c.mode()\n",
    "            else:\n",
    "                # Text input is list\n",
    "                if type(c) == list and len(c) == 1:\n",
    "                    c = self.cond_stage_model([c[0], c[0]])\n",
    "                    c = c[0:1]\n",
    "                else:\n",
    "                    c = self.cond_stage_model(c)\n",
    "        else:\n",
    "            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n",
    "            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n",
    "        return c\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_input(\n",
    "        self,\n",
    "        batch,\n",
    "        k,\n",
    "        return_first_stage_encode=True,\n",
    "        return_first_stage_outputs=False,\n",
    "        force_c_encode=False,\n",
    "        cond_key=None,\n",
    "        return_original_cond=False,\n",
    "        bs=None,\n",
    "    ):\n",
    "        x = super().get_input(batch, k)\n",
    "\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        if return_first_stage_encode:\n",
    "            encoder_posterior = self.encode_first_stage(x)\n",
    "            z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "        else:\n",
    "            z = None\n",
    "\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if cond_key is None:\n",
    "                cond_key = self.cond_stage_key\n",
    "            if cond_key != self.first_stage_key:\n",
    "                if cond_key in [\"caption\", \"coordinates_bbox\"]:\n",
    "                    xc = batch[cond_key]\n",
    "                elif cond_key == \"class_label\":\n",
    "                    xc = batch\n",
    "                else:\n",
    "                    # [bs, 1, 527]\n",
    "                    xc = super().get_input(batch, cond_key)\n",
    "                    if type(xc) == torch.Tensor:\n",
    "                        xc = xc.to(self.device)\n",
    "            else:\n",
    "                xc = x\n",
    "            if not self.cond_stage_trainable or force_c_encode:\n",
    "                if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                    c = self.get_learned_conditioning(xc)\n",
    "                else:\n",
    "                    c = self.get_learned_conditioning(xc.to(self.device))\n",
    "            else:\n",
    "                c = xc\n",
    "\n",
    "            if bs is not None:\n",
    "                c = c[:bs]\n",
    "\n",
    "        else:\n",
    "            c = None\n",
    "            xc = None\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                c = {\"pos_x\": pos_x, \"pos_y\": pos_y}\n",
    "        out = [z, c]\n",
    "        if return_first_stage_outputs:\n",
    "            xrec = self.decode_first_stage(z)\n",
    "            out.extend([x, xrec])\n",
    "        if return_original_cond:\n",
    "            out.append(xc)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, \"b h w c -> b c h w\").contiguous()\n",
    "\n",
    "        z = 1.0 / self.scale_factor * z\n",
    "        return self.first_stage_model.decode(z)\n",
    "\n",
    "    def mel_spectrogram_to_waveform(self, mel):\n",
    "        # Mel: [bs, 1, t-steps, fbins]\n",
    "        if len(mel.size()) == 4:\n",
    "            mel = mel.squeeze(1)\n",
    "        mel = mel.permute(0, 2, 1)\n",
    "        waveform = self.first_stage_model.vocoder(mel)\n",
    "        waveform = waveform.cpu().detach().numpy()\n",
    "        return waveform\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_first_stage(self, x):\n",
    "        return self.first_stage_model.encode(x)\n",
    "\n",
    "    def apply_model(self, x_noisy, t, cond, return_ids=False):\n",
    "\n",
    "        if isinstance(cond, dict):\n",
    "            # hybrid case, cond is exptected to be a dict\n",
    "            pass\n",
    "        else:\n",
    "            if not isinstance(cond, list):\n",
    "                cond = [cond]\n",
    "            if self.model.conditioning_key == \"concat\":\n",
    "                key = \"c_concat\"\n",
    "            elif self.model.conditioning_key == \"crossattn\":\n",
    "                key = \"c_crossattn\"\n",
    "            else:\n",
    "                key = \"c_film\"\n",
    "\n",
    "            cond = {key: cond}\n",
    "\n",
    "        x_recon = self.model(x_noisy, t, **cond)\n",
    "\n",
    "        if isinstance(x_recon, tuple) and not return_ids:\n",
    "            return x_recon[0]\n",
    "        else:\n",
    "            return x_recon\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self,\n",
    "        x,\n",
    "        c,\n",
    "        t,\n",
    "        clip_denoised: bool,\n",
    "        return_codebook_ids=False,\n",
    "        quantize_denoised=False,\n",
    "        return_x0=False,\n",
    "        score_corrector=None,\n",
    "        corrector_kwargs=None,\n",
    "    ):\n",
    "        t_in = t\n",
    "        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n",
    "\n",
    "        if score_corrector is not None:\n",
    "            assert self.parameterization == \"eps\"\n",
    "            model_out = score_corrector.modify_score(\n",
    "                self, model_out, x, t, c, **corrector_kwargs\n",
    "            )\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            model_out, logits = model_out\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1.0, 1.0)\n",
    "        if quantize_denoised:\n",
    "            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n",
    "            x_start=x_recon, x_t=x, t=t\n",
    "        )\n",
    "        if return_codebook_ids:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, logits\n",
    "        elif return_x0:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, x_recon\n",
    "        else:\n",
    "            return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        x,\n",
    "        c,\n",
    "        t,\n",
    "        clip_denoised=False,\n",
    "        repeat_noise=False,\n",
    "        return_codebook_ids=False,\n",
    "        quantize_denoised=False,\n",
    "        return_x0=False,\n",
    "        temperature=1.0,\n",
    "        noise_dropout=0.0,\n",
    "        score_corrector=None,\n",
    "        corrector_kwargs=None,\n",
    "    ):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        outputs = self.p_mean_variance(\n",
    "            x=x,\n",
    "            c=c,\n",
    "            t=t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            return_codebook_ids=return_codebook_ids,\n",
    "            quantize_denoised=quantize_denoised,\n",
    "            return_x0=return_x0,\n",
    "            score_corrector=score_corrector,\n",
    "            corrector_kwargs=corrector_kwargs,\n",
    "        )\n",
    "        if return_codebook_ids:\n",
    "            raise DeprecationWarning(\"Support dropped.\")\n",
    "            model_mean, _, model_log_variance, logits = outputs\n",
    "        elif return_x0:\n",
    "            model_mean, _, model_log_variance, x0 = outputs\n",
    "        else:\n",
    "            model_mean, _, model_log_variance = outputs\n",
    "\n",
    "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        if noise_dropout > 0.0:\n",
    "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (\n",
    "            (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1))).contiguous()\n",
    "        )\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            return model_mean + nonzero_mask * (\n",
    "                0.5 * model_log_variance\n",
    "            ).exp() * noise, logits.argmax(dim=1)\n",
    "        if return_x0:\n",
    "            return (\n",
    "                model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise,\n",
    "                x0,\n",
    "            )\n",
    "        else:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def progressive_denoising(\n",
    "        self,\n",
    "        cond,\n",
    "        shape,\n",
    "        verbose=True,\n",
    "        callback=None,\n",
    "        quantize_denoised=False,\n",
    "        img_callback=None,\n",
    "        mask=None,\n",
    "        x0=None,\n",
    "        temperature=1.0,\n",
    "        noise_dropout=0.0,\n",
    "        score_corrector=None,\n",
    "        corrector_kwargs=None,\n",
    "        batch_size=None,\n",
    "        x_T=None,\n",
    "        start_T=None,\n",
    "        log_every_t=None,\n",
    "    ):\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        timesteps = self.num_timesteps\n",
    "        if batch_size is not None:\n",
    "            b = batch_size if batch_size is not None else shape[0]\n",
    "            shape = [batch_size] + list(shape)\n",
    "        else:\n",
    "            b = batch_size = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=self.device)\n",
    "        else:\n",
    "            img = x_T\n",
    "        intermediates = []\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {\n",
    "                    key: cond[key][:batch_size]\n",
    "                    if not isinstance(cond[key], list)\n",
    "                    else list(map(lambda x: x[:batch_size], cond[key]))\n",
    "                    for key in cond\n",
    "                }\n",
    "            else:\n",
    "                cond = (\n",
    "                    [c[:batch_size] for c in cond]\n",
    "                    if isinstance(cond, list)\n",
    "                    else cond[:batch_size]\n",
    "                )\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = (\n",
    "            tqdm(\n",
    "                reversed(range(0, timesteps)),\n",
    "                desc=\"Progressive Generation\",\n",
    "                total=timesteps,\n",
    "            )\n",
    "            if verbose\n",
    "            else reversed(range(0, timesteps))\n",
    "        )\n",
    "        if type(temperature) == float:\n",
    "            temperature = [temperature] * timesteps\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != \"hybrid\"\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img, x0_partial = self.p_sample(\n",
    "                img,\n",
    "                cond,\n",
    "                ts,\n",
    "                clip_denoised=self.clip_denoised,\n",
    "                quantize_denoised=quantize_denoised,\n",
    "                return_x0=True,\n",
    "                temperature=temperature[i],\n",
    "                noise_dropout=noise_dropout,\n",
    "                score_corrector=score_corrector,\n",
    "                corrector_kwargs=corrector_kwargs,\n",
    "            )\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1.0 - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(x0_partial)\n",
    "            if callback:\n",
    "                callback(i)\n",
    "            if img_callback:\n",
    "                img_callback(img, i)\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        cond,\n",
    "        shape,\n",
    "        return_intermediates=False,\n",
    "        x_T=None,\n",
    "        verbose=True,\n",
    "        callback=None,\n",
    "        timesteps=None,\n",
    "        quantize_denoised=False,\n",
    "        mask=None,\n",
    "        x0=None,\n",
    "        img_callback=None,\n",
    "        start_T=None,\n",
    "        log_every_t=None,\n",
    "    ):\n",
    "\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        intermediates = [img]\n",
    "        if timesteps is None:\n",
    "            timesteps = self.num_timesteps\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = (\n",
    "            tqdm(reversed(range(0, timesteps)), desc=\"Sampling t\", total=timesteps)\n",
    "            if verbose\n",
    "            else reversed(range(0, timesteps))\n",
    "        )\n",
    "\n",
    "        if mask is not None:\n",
    "            assert x0 is not None\n",
    "            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != \"hybrid\"\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img = self.p_sample(\n",
    "                img,\n",
    "                cond,\n",
    "                ts,\n",
    "                clip_denoised=self.clip_denoised,\n",
    "                quantize_denoised=quantize_denoised,\n",
    "            )\n",
    "            if mask is not None:\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1.0 - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "            if callback:\n",
    "                callback(i)\n",
    "            if img_callback:\n",
    "                img_callback(img, i)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(\n",
    "        self,\n",
    "        cond,\n",
    "        batch_size=16,\n",
    "        return_intermediates=False,\n",
    "        x_T=None,\n",
    "        verbose=True,\n",
    "        timesteps=None,\n",
    "        quantize_denoised=False,\n",
    "        mask=None,\n",
    "        x0=None,\n",
    "        shape=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if shape is None:\n",
    "            shape = (batch_size, self.channels, self.latent_t_size, self.latent_f_size)\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {\n",
    "                    key: cond[key][:batch_size]\n",
    "                    if not isinstance(cond[key], list)\n",
    "                    else list(map(lambda x: x[:batch_size], cond[key]))\n",
    "                    for key in cond\n",
    "                }\n",
    "            else:\n",
    "                cond = (\n",
    "                    [c[:batch_size] for c in cond]\n",
    "                    if isinstance(cond, list)\n",
    "                    else cond[:batch_size]\n",
    "                )\n",
    "        return self.p_sample_loop(\n",
    "            cond,\n",
    "            shape,\n",
    "            return_intermediates=return_intermediates,\n",
    "            x_T=x_T,\n",
    "            verbose=verbose,\n",
    "            timesteps=timesteps,\n",
    "            quantize_denoised=quantize_denoised,\n",
    "            mask=mask,\n",
    "            x0=x0,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_log(\n",
    "        self,\n",
    "        cond,\n",
    "        batch_size,\n",
    "        ddim,\n",
    "        ddim_steps,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "        use_plms=False,\n",
    "        mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if mask is not None:\n",
    "            shape = (self.channels, mask.size()[-2], mask.size()[-1])\n",
    "        else:\n",
    "            shape = (self.channels, self.latent_t_size, self.latent_f_size)\n",
    "\n",
    "        intermediate = None\n",
    "        if ddim and not use_plms:\n",
    "            # print(\"Use ddim sampler\")\n",
    "\n",
    "            ddim_sampler = DDIMSampler(self)\n",
    "            samples, intermediates = ddim_sampler.sample(\n",
    "                ddim_steps,\n",
    "                batch_size,\n",
    "                shape,\n",
    "                cond,\n",
    "                verbose=False,\n",
    "                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                unconditional_conditioning=unconditional_conditioning,\n",
    "                mask=mask,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # print(\"Use DDPM sampler\")\n",
    "            samples, intermediates = self.sample(\n",
    "                cond=cond,\n",
    "                batch_size=batch_size,\n",
    "                return_intermediates=True,\n",
    "                unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                mask=mask,\n",
    "                unconditional_conditioning=unconditional_conditioning,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return samples, intermediate\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_sample(\n",
    "        self,\n",
    "        batchs,\n",
    "        ddim_steps=200,\n",
    "        ddim_eta=1.0,\n",
    "        x_T=None,\n",
    "        n_candidate_gen_per_text=1,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "        name=\"waveform\",\n",
    "        use_plms=False,\n",
    "        save=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Generate n_candidate_gen_per_text times and select the best\n",
    "        # Batch: audio, text, fnames\n",
    "        assert x_T is None\n",
    "        try:\n",
    "            batchs = iter(batchs)\n",
    "        except TypeError:\n",
    "            raise ValueError(\"The first input argument should be an iterable object\")\n",
    "\n",
    "        if use_plms:\n",
    "            assert ddim_steps is not None\n",
    "        use_ddim = ddim_steps is not None\n",
    "        # waveform_save_path = os.path.join(self.get_log_dir(), name)\n",
    "        # os.makedirs(waveform_save_path, exist_ok=True)\n",
    "        # print(\"Waveform save path: \", waveform_save_path)\n",
    "\n",
    "        with self.ema_scope(\"Generate\"):\n",
    "            for batch in batchs:\n",
    "                z, c = self.get_input(\n",
    "                    batch,\n",
    "                    self.first_stage_key,\n",
    "                    cond_key=self.cond_stage_key,\n",
    "                    return_first_stage_outputs=False,\n",
    "                    force_c_encode=True,\n",
    "                    return_original_cond=False,\n",
    "                    bs=None,\n",
    "                )\n",
    "                text = super().get_input(batch, \"text\")\n",
    "\n",
    "                # Generate multiple samples\n",
    "                batch_size = z.shape[0] * n_candidate_gen_per_text\n",
    "                c = torch.cat([c] * n_candidate_gen_per_text, dim=0)\n",
    "                text = text * n_candidate_gen_per_text\n",
    "\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    unconditional_conditioning = (\n",
    "                        self.cond_stage_model.get_unconditional_condition(batch_size)\n",
    "                    )\n",
    "\n",
    "                samples, _ = self.sample_log(\n",
    "                    cond=c,\n",
    "                    batch_size=batch_size,\n",
    "                    x_T=x_T,\n",
    "                    ddim=use_ddim,\n",
    "                    ddim_steps=ddim_steps,\n",
    "                    eta=ddim_eta,\n",
    "                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                    unconditional_conditioning=unconditional_conditioning,\n",
    "                    use_plms=use_plms,\n",
    "                )\n",
    "                \n",
    "                if(torch.max(torch.abs(samples)) > 1e2):\n",
    "                    samples = torch.clip(samples, min=-10, max=10)\n",
    "                    \n",
    "                mel = self.decode_first_stage(samples)\n",
    "\n",
    "                waveform = self.mel_spectrogram_to_waveform(mel)\n",
    "\n",
    "                if waveform.shape[0] > 1:\n",
    "                    similarity = self.cond_stage_model.cos_similarity(\n",
    "                        torch.FloatTensor(waveform).squeeze(1), text\n",
    "                    )\n",
    "\n",
    "                    best_index = []\n",
    "                    for i in range(z.shape[0]):\n",
    "                        candidates = similarity[i :: z.shape[0]]\n",
    "                        max_index = torch.argmax(candidates).item()\n",
    "                        best_index.append(i + max_index * z.shape[0])\n",
    "\n",
    "                    waveform = waveform[best_index]\n",
    "                    # print(\"Similarity between generated audio and text\", similarity)\n",
    "                    # print(\"Choose the following indexes:\", best_index)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_sample_masked(\n",
    "        self,\n",
    "        batchs,\n",
    "        ddim_steps=200,\n",
    "        ddim_eta=1.0,\n",
    "        x_T=None,\n",
    "        n_candidate_gen_per_text=1,\n",
    "        unconditional_guidance_scale=1.0,\n",
    "        unconditional_conditioning=None,\n",
    "        name=\"waveform\",\n",
    "        use_plms=False,\n",
    "        time_mask_ratio_start_and_end=(0.25, 0.75),\n",
    "        freq_mask_ratio_start_and_end=(0.75, 1.0),\n",
    "        save=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Generate n_candidate_gen_per_text times and select the best\n",
    "        # Batch: audio, text, fnames\n",
    "        assert x_T is None\n",
    "        try:\n",
    "            batchs = iter(batchs)\n",
    "        except TypeError:\n",
    "            raise ValueError(\"The first input argument should be an iterable object\")\n",
    "\n",
    "        if use_plms:\n",
    "            assert ddim_steps is not None\n",
    "        use_ddim = ddim_steps is not None\n",
    "        # waveform_save_path = os.path.join(self.get_log_dir(), name)\n",
    "        # os.makedirs(waveform_save_path, exist_ok=True)\n",
    "        # print(\"Waveform save path: \", waveform_save_path)\n",
    "\n",
    "        with self.ema_scope(\"Generate\"):\n",
    "            for batch in batchs:\n",
    "                z, c = self.get_input(\n",
    "                    batch,\n",
    "                    self.first_stage_key,\n",
    "                    cond_key=self.cond_stage_key,\n",
    "                    return_first_stage_outputs=False,\n",
    "                    force_c_encode=True,\n",
    "                    return_original_cond=False,\n",
    "                    bs=None,\n",
    "                )\n",
    "                text = super().get_input(batch, \"text\")\n",
    "                \n",
    "                # Generate multiple samples\n",
    "                batch_size = z.shape[0] * n_candidate_gen_per_text\n",
    "                \n",
    "                _, h, w = z.shape[0], z.shape[2], z.shape[3]\n",
    "                \n",
    "                mask = torch.ones(batch_size, h, w).to(self.device)\n",
    "                \n",
    "                mask[:, int(h * time_mask_ratio_start_and_end[0]) : int(h * time_mask_ratio_start_and_end[1]), :] = 0 \n",
    "                mask[:, :, int(w * freq_mask_ratio_start_and_end[0]) : int(w * freq_mask_ratio_start_and_end[1])] = 0 \n",
    "                mask = mask[:, None, ...]\n",
    "                \n",
    "                c = torch.cat([c] * n_candidate_gen_per_text, dim=0)\n",
    "                text = text * n_candidate_gen_per_text\n",
    "\n",
    "                if unconditional_guidance_scale != 1.0:\n",
    "                    unconditional_conditioning = (\n",
    "                        self.cond_stage_model.get_unconditional_condition(batch_size)\n",
    "                    )\n",
    "\n",
    "                samples, _ = self.sample_log(\n",
    "                    cond=c,\n",
    "                    batch_size=batch_size,\n",
    "                    x_T=x_T,\n",
    "                    ddim=use_ddim,\n",
    "                    ddim_steps=ddim_steps,\n",
    "                    eta=ddim_eta,\n",
    "                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                    unconditional_conditioning=unconditional_conditioning,\n",
    "                    use_plms=use_plms, mask=mask, x0=torch.cat([z] * n_candidate_gen_per_text)\n",
    "                )\n",
    "\n",
    "                mel = self.decode_first_stage(samples)\n",
    "\n",
    "                waveform = self.mel_spectrogram_to_waveform(mel)\n",
    "\n",
    "                if waveform.shape[0] > 1:\n",
    "                    similarity = self.cond_stage_model.cos_similarity(\n",
    "                        torch.FloatTensor(waveform).squeeze(1), text\n",
    "                    )\n",
    "\n",
    "                    best_index = []\n",
    "                    for i in range(z.shape[0]):\n",
    "                        candidates = similarity[i :: z.shape[0]]\n",
    "                        max_index = torch.argmax(candidates).item()\n",
    "                        best_index.append(i + max_index * z.shape[0])\n",
    "\n",
    "                    waveform = waveform[best_index]\n",
    "                    # print(\"Similarity between generated audio and text\", similarity)\n",
    "                    # print(\"Choose the following indexes:\", best_index)\n",
    "\n",
    "        return waveform"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
